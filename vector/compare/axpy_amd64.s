// Code generated by command: go run asm.go -out axpy_amd64.s -stubs axpy_amd64.go -testhelp axpy_amd64_stub_test.go. DO NOT EDIT.

#include "textflag.h"

// func AxpyPointer_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointerLoop_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V0A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V1A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V2A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V3A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafe_V4A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V0A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V0A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V1A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V1A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V2A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V2A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V3A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V3A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeInterleave_V4A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeInterleave_V4A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET
