// Code generated by command: go run main.go -out axpy_amd64.s -stubs axpy_amd64.go -testhelp axpy_stub_amd64_test.go. DO NOT EDIT.

#include "textflag.h"

// func AmdAxpyPointer_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointer_V5A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointer_V5A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoop_V5A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoop_V5A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V0A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V0A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V1A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V1A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V2A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V2A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V3A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V3A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V4A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V4A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeX_V5A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeX_V5A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  CX, DI
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V0A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V0A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V1A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V1A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V2A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V2A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V3A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V3A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V4A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V4A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyUnsafeXInterleave_V5A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyUnsafeXInterleave_V5A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X2
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X3
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X4
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X5
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X6
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X7
	ADDQ  CX, DI
	MOVSS (AX)(DI*4), X8
	ADDQ  CX, DI
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X2
	MOVSS X2, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X3
	MOVSS X3, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X4
	MOVSS X4, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X5
	MOVSS X5, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X6
	MOVSS X6, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X7
	MOVSS X7, (DX)(R8*4)
	ADDQ  BX, R8
	ADDSS (DX)(R8*4), X8
	MOVSS X8, (DX)(R8*4)
	ADDQ  BX, R8
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V0A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V0A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V1A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V1A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V2A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V2A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V3A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V3A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V4A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V4A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopX_V5A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopX_V5A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x04, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x04, SI

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V0A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V0A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V1A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V1A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V2A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V2A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V3A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V3A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V4A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V4A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AmdAxpyPointerLoopXInterleave_V5A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AmdAxpyPointerLoopXInterleave_V5A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  BX, SI
	SHLQ  $0x05, SI
	MOVQ  n+40(FP), SI
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X2
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X3
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X4
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X5
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X6
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X7
	LEAQ  (AX)(CX*4), AX
	MOVSS (AX), X8
	LEAQ  (AX)(CX*4), AX
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X2
	MOVSS X2, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X3
	MOVSS X3, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X4
	MOVSS X4, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X5
	MOVSS X5, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X6
	MOVSS X6, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X7
	MOVSS X7, (DX)
	LEAQ  (DX)(BX*4), DX
	ADDSS (DX), X8
	MOVSS X8, (DX)
	LEAQ  (DX)(BX*4), DX
	SUBQ  $0x08, SI

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET
