// Code generated by command: go run asm.go -out axpy_amd64.s -stubs axpy_amd64.go -testhelp axpy_amd64_stub_test.go. DO NOT EDIT.

#include "textflag.h"

// func AxpyPointer_A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointerLoop_A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET
