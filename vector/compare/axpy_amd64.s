// Code generated by command: go run asm.go -out axpy_amd64.s -stubs axpy_amd64.go -testhelp axpy_amd64_stub_test.go. DO NOT EDIT.

#include "textflag.h"

// func AxpyPointer_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointerLoop_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A1(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A1(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A2(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A2(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A3(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A3(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A5(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A5(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A6(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A6(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A7(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A7(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyUnsafe_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafe_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	XORQ  R9, R9
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(R8*4), X1
	MULSS X0, X1
	ADDSS (DX)(R9*4), X1
	MOVSS X1, (DX)(R9*4)
	INCQ  DI
	ADDQ  CX, R8
	ADDQ  BX, R9

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET
