// Code generated by command: go run asm.go -out axpy_amd64.s -stubs axpy_amd64.go -testhelp axpy_amd64_stub_test.go. DO NOT EDIT.

#include "textflag.h"

// func AxpyPointer_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointer_V5A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointer_V5A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	SHLQ  $0x02, SI
	IMULQ CX, SI
	ADDQ  AX, SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, AX
	JHI  loop
	RET

// func AxpyPointerLoop_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoop_V5A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoop_V5A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	INCQ  DI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, DI
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (DX), X1
	MOVSS X1, (DX)
	DECQ  SI
	LEAQ  (AX)(CX*4), AX
	LEAQ  (DX)(BX*4), DX

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A0(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A0(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A9(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A9(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A10(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A10(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A11(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A11(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A12(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A12(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A13(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A13(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A14(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A14(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A15(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A15(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A16(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A16(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit
	PCALIGN $0x10

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V0A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V0A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V1A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V1A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V2A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V2A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V3A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V3A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V4A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V4A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeX_V5A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeX_V5A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	MOVSS 4(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 4(DX)(R8*4), X1
	MOVSS X1, 4(DX)(R8*4)
	MOVSS 8(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 8(DX)(R8*4), X1
	MOVSS X1, 8(DX)(R8*4)
	MOVSS 12(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 12(DX)(R8*4), X1
	MOVSS X1, 12(DX)(R8*4)
	MOVSS 16(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 16(DX)(R8*4), X1
	MOVSS X1, 16(DX)(R8*4)
	MOVSS 20(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 20(DX)(R8*4), X1
	MOVSS X1, 20(DX)(R8*4)
	MOVSS 24(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 24(DX)(R8*4), X1
	MOVSS X1, 24(DX)(R8*4)
	MOVSS 28(AX)(DI*4), X1
	MULSS X0, X1
	ADDSS 28(DX)(R8*4), X1
	MOVSS X1, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHI  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A0R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A0R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A8R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A8R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A9R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A9R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A10R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A10R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A11R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A11R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A12R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A12R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A13R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A13R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A14R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A14R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A15R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A15R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A16R4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A16R4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	SUBQ  $0x04, SI
	LEAQ  (DI)(CX*4), DI
	LEAQ  (R8)(BX*4), R8

check_limit_unroll:
	CMPQ SI, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A0R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A0R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A8R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A8R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A9R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A9R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A10R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A10R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A11R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A11R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A12R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A12R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A13R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A13R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A14R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A14R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A15R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A15R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V0A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V0A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V1A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V1A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V2A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V2A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V3A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V3A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V4A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V4A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyUnsafeXInterleave_V5A16R8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyUnsafeXInterleave_V5A16R8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  ys+24(FP), DX
	MOVQ  incy+32(FP), BX
	MOVQ  n+40(FP), SI
	XORQ  DI, DI
	XORQ  R8, R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX)(DI*4), X1
	MOVSS 4(AX)(DI*4), X2
	MOVSS 8(AX)(DI*4), X3
	MOVSS 12(AX)(DI*4), X4
	MOVSS 16(AX)(DI*4), X5
	MOVSS 20(AX)(DI*4), X6
	MOVSS 24(AX)(DI*4), X7
	MOVSS 28(AX)(DI*4), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (DX)(R8*4), X1
	ADDSS 4(DX)(R8*4), X2
	ADDSS 8(DX)(R8*4), X3
	ADDSS 12(DX)(R8*4), X4
	ADDSS 16(DX)(R8*4), X5
	ADDSS 20(DX)(R8*4), X6
	ADDSS 24(DX)(R8*4), X7
	ADDSS 28(DX)(R8*4), X8
	MOVSS X1, (DX)(R8*4)
	MOVSS X2, 4(DX)(R8*4)
	MOVSS X3, 8(DX)(R8*4)
	MOVSS X4, 12(DX)(R8*4)
	MOVSS X5, 16(DX)(R8*4)
	MOVSS X6, 20(DX)(R8*4)
	MOVSS X7, 24(DX)(R8*4)
	MOVSS X8, 28(DX)(R8*4)
	SUBQ  $0x08, SI
	LEAQ  (DI)(CX*8), DI
	LEAQ  (R8)(BX*8), R8

check_limit_unroll:
	CMPQ SI, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX)(DI*4), X1
	MULSS X0, X1
	ADDSS (DX)(R8*4), X1
	MOVSS X1, (DX)(R8*4)
	DECQ  SI
	ADDQ  CX, DI
	ADDQ  BX, R8

check_limit:
	CMPQ SI, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V0A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V0A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V1A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V1A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V2A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V2A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V3A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V3A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V4A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V4A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopX_V5A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopX_V5A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	MOVSS 4(AX), X1
	MULSS X0, X1
	ADDSS 4(BX), X1
	MOVSS X1, 4(BX)
	MOVSS 8(AX), X1
	MULSS X0, X1
	ADDSS 8(BX), X1
	MOVSS X1, 8(BX)
	MOVSS 12(AX), X1
	MULSS X0, X1
	ADDSS 12(BX), X1
	MOVSS X1, 12(BX)
	MOVSS 16(AX), X1
	MULSS X0, X1
	ADDSS 16(BX), X1
	MOVSS X1, 16(BX)
	MOVSS 20(AX), X1
	MULSS X0, X1
	ADDSS 20(BX), X1
	MOVSS X1, 20(BX)
	MOVSS 24(AX), X1
	MULSS X0, X1
	ADDSS 24(BX), X1
	MOVSS X1, 24(BX)
	MOVSS 28(AX), X1
	MULSS X0, X1
	ADDSS 28(BX), X1
	MOVSS X1, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A0U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A0U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A8U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A8U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A9U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A9U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A10U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A10U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A11U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A11U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A12U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A12U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A13U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A13U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A14U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A14U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A15U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A15U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A16U4(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A16U4(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x04, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x04, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	SUBQ  $0x04, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x04
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A0U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A0U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A8U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A8U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A9U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A9U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A10U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A10U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A11U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A11U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A12U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A12U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A13U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A13U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A14U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A14U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A15U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A15U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x08
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP
	NOP

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V0A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V0A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V1A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V1A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V2A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V2A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V3A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V3A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V4A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V4A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET

// func AxpyPointerLoopXInterleave_V5A16U8(alpha float32, xs *float32, incx uintptr, ys *float32, incy uintptr, n uintptr)
// Requires: SSE
TEXT ·AxpyPointerLoopXInterleave_V5A16U8(SB), NOSPLIT, $0-48
	MOVSS alpha+0(FP), X0
	MOVQ  xs+8(FP), AX
	MOVQ  incx+16(FP), CX
	MOVQ  CX, DX
	SHLQ  $0x05, DX
	MOVQ  ys+24(FP), BX
	MOVQ  incy+32(FP), SI
	MOVQ  SI, DI
	SHLQ  $0x05, DI
	MOVQ  n+40(FP), R8
	JMP   check_limit_unroll
	PCALIGN $0x10

loop_unroll:
	MOVSS (AX), X1
	MOVSS 4(AX), X2
	MOVSS 8(AX), X3
	MOVSS 12(AX), X4
	MOVSS 16(AX), X5
	MOVSS 20(AX), X6
	MOVSS 24(AX), X7
	MOVSS 28(AX), X8
	MULSS X0, X1
	MULSS X0, X2
	MULSS X0, X3
	MULSS X0, X4
	MULSS X0, X5
	MULSS X0, X6
	MULSS X0, X7
	MULSS X0, X8
	ADDSS (BX), X1
	ADDSS 4(BX), X2
	ADDSS 8(BX), X3
	ADDSS 12(BX), X4
	ADDSS 16(BX), X5
	ADDSS 20(BX), X6
	ADDSS 24(BX), X7
	ADDSS 28(BX), X8
	MOVSS X1, (BX)
	MOVSS X2, 4(BX)
	MOVSS X3, 8(BX)
	MOVSS X4, 12(BX)
	MOVSS X5, 16(BX)
	MOVSS X6, 20(BX)
	MOVSS X7, 24(BX)
	MOVSS X8, 28(BX)
	SUBQ  $0x08, R8
	ADDQ  DX, AX
	ADDQ  DI, BX

check_limit_unroll:
	CMPQ R8, $0x08
	JHS  loop_unroll
	JMP  check_limit

loop:
	MOVSS (AX), X1
	MULSS X0, X1
	ADDSS (BX), X1
	MOVSS X1, (BX)
	DECQ  R8
	LEAQ  (AX)(CX*4), AX
	LEAQ  (BX)(SI*4), BX

check_limit:
	CMPQ R8, $0x00
	JHI  loop
	RET
